{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Nearest Neighbor with KD Tree\n",
    "As a lazy learner, kNN defers all the computations of a Nearest Neighbor Search to inference step, and thus using a Nearest Neighbor Search approach for classification using kNN has an expensive inference time complexity of **O(m * log k)** where m is the # of training samples and k is the number of neighbors to consider for NN Search. As such, Kd tree emerges as an alternative such that some prior computation is done at training time. Notice the uppercase in Kd tree as compared to the lowercase of kNN.<br><br>\n",
    "This notebook implements KD Tree to search 1-NN to classify the handwritten dataset available [here](https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from math import floor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "root_dir_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "sys.path.append(root_dir_path)\n",
    "import utils\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13., 13.,  8.,  0.,  0.,  0.,  0., 16., 11., 13.,\n",
       "        16.,  6.,  0.,  0.,  1., 16.,  5.,  2., 14.,  9.,  0.,  0.,  0.,\n",
       "         9., 16., 16., 15.,  0.,  0.,  0.,  0., 10., 16., 14., 14.,  0.,\n",
       "         0.,  0.,  5., 15.,  4.,  0., 16.,  6.,  0.,  0.,  6., 14.,  7.,\n",
       "         6., 16.,  4.,  0.,  0.,  0.,  7., 15., 16., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  3., 14., 16., 14.,  0.,  0.,  0.,  0., 13., 13., 13.,\n",
       "        16.,  2.,  0.,  0.,  0.,  1.,  0.,  9., 15.,  0.,  0.,  0.,  0.,\n",
       "         9., 12., 15., 16., 10.,  0.,  0.,  4., 16., 16., 16., 11.,  3.,\n",
       "         0.,  0.,  0.,  4.,  9., 14.,  2.,  0.,  0.,  0.,  0.,  2., 15.,\n",
       "         9.,  0.,  0.,  0.,  0.,  0.,  4., 13.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "digits_data = load_digits()\n",
    "X = digits_data.data\n",
    "y = digits_data.target\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "X_train[0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1257, 64), (540, 64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 occurs 125 time(s).\n",
      "1 occurs 132 time(s).\n",
      "2 occurs 130 time(s).\n",
      "3 occurs 129 time(s).\n",
      "4 occurs 121 time(s).\n",
      "5 occurs 116 time(s).\n",
      "6 occurs 128 time(s).\n",
      "7 occurs 124 time(s).\n",
      "8 occurs 131 time(s).\n",
      "9 occurs 121 time(s).\n"
     ]
    }
   ],
   "source": [
    "# unique values and their counts\n",
    "unique_values, counts = np.unique(y_train, return_counts=True)\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"{value} occurs {count} time(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.3125    , 0.8125    , 0.8125    ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.6875    , 0.8125    , 1.        , 0.375     ,\n",
       "        0.        , 0.        , 0.0625    , 1.        , 0.3125    ,\n",
       "        0.125     , 0.875     , 0.5625    , 0.        , 0.        ,\n",
       "        0.        , 0.5625    , 1.        , 1.        , 0.9375    ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.625     ,\n",
       "        1.        , 0.875     , 0.875     , 0.        , 0.        ,\n",
       "        0.        , 0.3125    , 0.9375    , 0.25      , 0.        ,\n",
       "        1.        , 0.375     , 0.        , 0.        , 0.375     ,\n",
       "        0.875     , 0.4375    , 0.375     , 1.        , 0.25      ,\n",
       "        0.        , 0.        , 0.        , 0.4375    , 0.9375    ,\n",
       "        1.        , 0.625     , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.1875    , 0.875     , 1.        ,\n",
       "        0.875     , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.8125    , 0.8125    , 0.8125    , 1.        , 0.125     ,\n",
       "        0.        , 0.        , 0.        , 0.0625    , 0.        ,\n",
       "        0.5625    , 0.9375    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5625    , 0.75      , 0.9375    , 1.        ,\n",
       "        0.66666667, 0.        , 0.        , 0.28571429, 1.        ,\n",
       "        1.        , 1.        , 0.6875    , 0.21428571, 0.        ,\n",
       "        0.        , 0.        , 0.25      , 0.5625    , 0.875     ,\n",
       "        0.125     , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.125     , 0.9375    , 0.5625    , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.25      , 0.8125    ,\n",
       "        0.0625    , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_training_features = utils.minmax_normalize_2d_array(X_train)\n",
    "normalized_test_features = utils.minmax_normalize_2d_array(X_test)\n",
    "X_train = None\n",
    "X_test = None\n",
    "normalized_training_features[0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_Node:\n",
    "    def __init__(self, X, y, depth):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.depth = depth\n",
    "        self.median_idx = None\n",
    "        self.tr_sample = None\n",
    "        self.label = None\n",
    "        self.dim_to_split = None\n",
    "\n",
    "    # Worst Case TC: O( m . (m^2)) = O(m^3)\n",
    "    # since bad pivot choice for median calculation sorting means height of tree ~m\n",
    "    # and worst TC for quicksort is O(m^2).\n",
    "    def build_Kd_tree(self):\n",
    "        \"\"\"\n",
    "        Builds a Kd tree rooted at this node.\n",
    "        \"\"\"\n",
    "\n",
    "        # check if the current node is a leaf node\n",
    "        if len(self.X) == 1:\n",
    "            self.tr_sample = self.X[0, :]\n",
    "            self.label = self.y[0]\n",
    "        else:\n",
    "            # find out the feature to calculate median\n",
    "            self.dim_to_split = self.depth % (len(self.X[0]))\n",
    "            # find median by sorting the tr samples  along feature\n",
    "            # TC: Average case: O(m log m) using quicksort.\n",
    "            # TC: Worst case: O(n^2) using quicksort.\n",
    "            sorted_indices = self.X[:, self.dim_to_split].argsort()\n",
    "            self.X = self.X[sorted_indices]\n",
    "            # also sort the training labels to the corrsponding indices\n",
    "            self.y = self.y[sorted_indices]\n",
    "            if len(self.X) % 2 == 0:\n",
    "                self.median_idx = int((len(self.X) / 2))\n",
    "            else:\n",
    "                self.median_idx = floor((len(self.X) / 2))\n",
    "\n",
    "            # store the median point and label\n",
    "            self.tr_sample = self.X[self.median_idx, :]\n",
    "            self.label = self.y[self.median_idx]\n",
    "            # separate left and right points\n",
    "            left_points = self.X[0 : self.median_idx, :]\n",
    "            left_labels = self.y[0 : self.median_idx]\n",
    "            right_points = self.X[self.median_idx + 1 :, :]\n",
    "            right_labels = self.y[self.median_idx + 1 :]\n",
    "            # create KD subtrees' nodes from the left and the right points\n",
    "            if len(left_points) == 0:\n",
    "                self.left = None\n",
    "            else:\n",
    "                self.left = Tree_Node(left_points, left_labels, self.depth + 1)\n",
    "                # build KD subtrees\n",
    "                self.left.build_Kd_tree()\n",
    "            if len(right_points) == 0:\n",
    "                self.right = None\n",
    "            else:\n",
    "                self.right = Tree_Node(right_points, right_labels, self.depth + 1)\n",
    "                # build KD subtree\n",
    "                self.right.build_Kd_tree()\n",
    "\n",
    "    # Worst TC: Might need to travel to all nodes when no pruning can be done. Thus, O(m)\n",
    "    def find_nearest_neighbor(\n",
    "        self, X_test, curr_nearest_dist, curr_nearest_sample, curr_nearest_label\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Finds the nearest neighbor of test sample X_test by traversing the tree rooted at this node.\n",
    "        \"\"\"\n",
    "\n",
    "        # check if the current node is the closest to the test sample till now\n",
    "        test_sample_and_curr_node_dist = np.sqrt(np.sum((X_test - self.tr_sample) ** 2))\n",
    "        if curr_nearest_sample is None or (\n",
    "            test_sample_and_curr_node_dist < curr_nearest_dist\n",
    "        ):\n",
    "            curr_nearest_dist = test_sample_and_curr_node_dist\n",
    "            curr_nearest_sample = self.tr_sample\n",
    "            curr_nearest_label = self.label\n",
    "\n",
    "        # traverse child nodes - find good and bad side to traverse first\n",
    "        if self.left is None and self.right is None:\n",
    "            # it is a leaf node\n",
    "            good_child = None\n",
    "            bad_child = None\n",
    "\n",
    "        else:\n",
    "            if X_test[self.dim_to_split] < self.tr_sample[self.dim_to_split]:\n",
    "                # test sample is to the left side of the splitting node\n",
    "                good_child = self.left\n",
    "                bad_child = self.right\n",
    "            else:\n",
    "                good_child = self.right\n",
    "                bad_child = self.left\n",
    "\n",
    "        # traverse good side first\n",
    "        if good_child is not None:\n",
    "            (\n",
    "                NN_dist_good_side,\n",
    "                NN_good_side,\n",
    "                NN_label_good_side,\n",
    "            ) = good_child.find_nearest_neighbor(\n",
    "                X_test, curr_nearest_dist, curr_nearest_sample, curr_nearest_label\n",
    "            )\n",
    "        else:\n",
    "            NN_dist_good_side, NN_good_side, NN_label_good_side = (\n",
    "                curr_nearest_dist,\n",
    "                curr_nearest_sample,\n",
    "                curr_nearest_label,\n",
    "            )\n",
    "\n",
    "        # traverse bad side second, but only if it is worth traversing\n",
    "        # do not traverse bad side if split dim diff between\n",
    "        # test sample and bad side child's split dim is greater than the currest best dist\n",
    "        if bad_child is not None and (\n",
    "            abs(X_test[self.dim_to_split] - self.tr_sample[self.dim_to_split])\n",
    "            < curr_nearest_dist\n",
    "        ):\n",
    "            (\n",
    "                NN_dist_bad_side,\n",
    "                NN_bad_side,\n",
    "                NN_label_bad_side,\n",
    "            ) = bad_child.find_nearest_neighbor(\n",
    "                X_test, curr_nearest_dist, curr_nearest_sample, curr_nearest_label\n",
    "            )\n",
    "        else:\n",
    "            NN_dist_bad_side, NN_bad_side, NN_label_bad_side = (\n",
    "                curr_nearest_dist,\n",
    "                curr_nearest_sample,\n",
    "                curr_nearest_label,\n",
    "            )\n",
    "\n",
    "        # choose between the left best and the right best\n",
    "        if NN_dist_good_side < NN_dist_bad_side:\n",
    "            curr_nearest_dist = NN_dist_good_side\n",
    "            curr_nearest_sample = NN_good_side\n",
    "            curr_nearest_label = NN_label_good_side\n",
    "        else:\n",
    "            curr_nearest_dist = NN_dist_bad_side\n",
    "            curr_nearest_sample = NN_bad_side\n",
    "            curr_nearest_label = NN_label_bad_side\n",
    "\n",
    "        return curr_nearest_dist, curr_nearest_sample, curr_nearest_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KD Tree Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build KD tree\n",
    "KD_tree = Tree_Node(normalized_training_features, y_train, 0)\n",
    "KD_tree.build_Kd_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "Search for one nearest neighbor of each test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_samples = []\n",
    "nearest_labels = []\n",
    "for idx in range(0, len(normalized_test_features)):\n",
    "    _, nearest_sample, nearest_label = KD_tree.find_nearest_neighbor(\n",
    "        normalized_test_features[idx, :], float(\"inf\"), None, None\n",
    "    )\n",
    "    nearest_samples.append(nearest_sample)\n",
    "    nearest_labels.append(nearest_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.51851851851852"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.calculate_accuracy(y_test, nearest_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the worst time complexity if O(m), Kd trees still perform well in practice since pruning is done to some degree in real applications. However, Kd tree struggles with the curse of dimensionality as the feature space dimensions increase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
