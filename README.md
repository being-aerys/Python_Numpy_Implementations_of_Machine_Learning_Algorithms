# Machine Learning Algorithms
This repo is my personal effort to implement several supervised and unsupervised learning algorithms using **only python and numpy**. While directly using ML libraries is efficient at prototyping, it skips the challenges that arise while implementing these algorithms - vectorization is one good example for this. This has inspired me to implement these algorithms and tackle these challenges first-hand. I have used some ML libraries but solely for auxiliary tasks like normalization. This is a work in progress and I will add more algorithms over time. <br>

Following is a projection onto top **3 eigen vectors** achieved after using **PCA** to reduce the data set from a 38-dimensional feature space.

<img src="https://github.com/being-aerys/Python_Numpy_Implementations_of_Machine_Learning_Algorithms/blob/master/Unsupervised_Methods/Dimension_Reduction/pca_3D.PNG" width="700" height="600">

## Dependencies
This project requires python and the following python libraries.
1. pandas
2. numpy
3. seaborn
4. matplotlib
5. scikit-learn

It also requires a software that can open and execute a Jupyter Notebook.


## Installation
1. Clone the repo or download using the url: https://github.com/intelaashish/Machine_Learning_Algorithms_Collection.git
2. Download the corresponding necessary data from the web for each jupyter notebook file.
3. Navigate to the  directory that contains the corresponding notebook.
4. Run the following command:
    ```properties
        jupyter notebook
     

6. This will open a tab on a web browser.
7. Click on the notebook file corresponding to the machine learning method that you want to explore.

## Algorithms Covered/ In Progress
### Supervised Algorithms
1. Linear Regression
2. Naive Bayes<br>
    a. Multinomial Naive Bayes<br>
    b. Gaussian Naive Bayes
3. Perceptron<br>
4. Support Vector Machines<br>
    a. Hard Margin SVM in Dual Form using QP Solver<br>
    b. Hard and Soft Margin SVM in Primal/ Natural form using Gradient Descent 

### Unsupervised Algorithms
1. Dimensionality Reduction<br>
    a. PCA<br>
    b. LDA





